{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75911ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riori\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\riori\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\riori\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "C:\\Users\\riori\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\riori\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance between cat features:\n",
      "Cat 1 vs Cat 2: 0.0042920708656311035\n",
      "Cat 1 vs Cat 3: 0.004217803478240967\n",
      "Cat 2 vs Cat 3: 0.004291892051696777\n",
      "\n",
      "L2 norm between cat features:\n",
      "Cat 1 vs Cat 2: 2.020491361618042\n",
      "Cat 1 vs Cat 3: 1.9662295579910278\n",
      "Cat 2 vs Cat 3: 2.0051419734954834\n",
      "\n",
      "Cosine distance between dog features:\n",
      "Dog 1 vs Dog 2: 0.0041792988777160645\n",
      "Dog 1 vs Dog 3: 0.00434952974319458\n",
      "Dog 2 vs Dog 3: 0.004631519317626953\n",
      "\n",
      "L2 norm between dog features:\n",
      "Dog 1 vs Dog 2: 1.9651762247085571\n",
      "Dog 1 vs Dog 3: 2.0222537517547607\n",
      "Dog 2 vs Dog 3: 2.1005399227142334\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "# Remove the last classification layer\n",
    "resnet50 = torch.nn.Sequential(*(list(resnet50.children())[:-1]))\n",
    "\n",
    "# Define ImageNet normalization parameters\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define transforms to resize, normalize, and convert images to tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load images of cats and dogs\n",
    "cat_images = []\n",
    "dog_images = []\n",
    "for i in range(1, 4):\n",
    "    cat_img = Image.open(f'cat-{i}.jpeg')\n",
    "    dog_img = Image.open(f'dog-{i}.jpeg')\n",
    "    cat_images.append(transform(cat_img))\n",
    "    dog_images.append(transform(dog_img))\n",
    "\n",
    "# Extract features for cat and dog images\n",
    "def extract_features(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        with torch.no_grad():\n",
    "            img = img.unsqueeze(0)\n",
    "            feature = resnet50(img)\n",
    "            feature = feature.squeeze().numpy()\n",
    "            features.append(feature)\n",
    "    return features\n",
    "\n",
    "cat_features = extract_features(cat_images)\n",
    "dog_features = extract_features(dog_images)\n",
    "\n",
    "# Compute cosine distance and L2 norm\n",
    "def cosine_distance(u, v):\n",
    "    return 1 - np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "\n",
    "def l2_norm(u, v):\n",
    "    return np.linalg.norm(u - v)\n",
    "\n",
    "# Compare cat features with each other\n",
    "print(\"Cosine distance between cat features:\")\n",
    "for i in range(len(cat_features)):\n",
    "    for j in range(i + 1, len(cat_features)):\n",
    "        dist = cosine_distance(cat_features[i], cat_features[j])\n",
    "        print(f\"Cat {i+1} vs Cat {j+1}: {dist}\")\n",
    "\n",
    "print(\"\\nL2 norm between cat features:\")\n",
    "for i in range(len(cat_features)):\n",
    "    for j in range(i + 1, len(cat_features)):\n",
    "        dist = l2_norm(cat_features[i], cat_features[j])\n",
    "        print(f\"Cat {i+1} vs Cat {j+1}: {dist}\")\n",
    "\n",
    "# Compare dog features with each other\n",
    "print(\"\\nCosine distance between dog features:\")\n",
    "for i in range(len(dog_features)):\n",
    "    for j in range(i + 1, len(dog_features)):\n",
    "        dist = cosine_distance(dog_features[i], dog_features[j])\n",
    "        print(f\"Dog {i+1} vs Dog {j+1}: {dist}\")\n",
    "\n",
    "print(\"\\nL2 norm between dog features:\")\n",
    "for i in range(len(dog_features)):\n",
    "    for j in range(i + 1, len(dog_features)):\n",
    "        dist = l2_norm(dog_features[i], dog_features[j])\n",
    "        print(f\"Dog {i+1} vs Dog {j+1}: {dist}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27f3c365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine distance between cat and dog features:\n",
      "Cat 1 vs Dog 1: 0.004583597183227539\n",
      "Cat 1 vs Dog 2: 0.004755914211273193\n",
      "Cat 1 vs Dog 3: 0.004564762115478516\n",
      "Cat 2 vs Dog 1: 0.004246354103088379\n",
      "Cat 2 vs Dog 2: 0.004286289215087891\n",
      "Cat 2 vs Dog 3: 0.004675447940826416\n",
      "Cat 3 vs Dog 1: 0.004382789134979248\n",
      "Cat 3 vs Dog 2: 0.004277467727661133\n",
      "Cat 3 vs Dog 3: 0.004632771015167236\n",
      "\n",
      "L2 norm between cat and dog features:\n",
      "Cat 1 vs Dog 1: 2.060588836669922\n",
      "Cat 1 vs Dog 2: 2.0776333808898926\n",
      "Cat 1 vs Dog 3: 2.0945916175842285\n",
      "Cat 2 vs Dog 1: 1.9914679527282715\n",
      "Cat 2 vs Dog 2: 2.0115714073181152\n",
      "Cat 2 vs Dog 3: 2.0948598384857178\n",
      "Cat 3 vs Dog 1: 2.0105555057525635\n",
      "Cat 3 vs Dog 2: 1.9789475202560425\n",
      "Cat 3 vs Dog 3: 2.091280221939087\n"
     ]
    }
   ],
   "source": [
    "# Compare cat features with dog features\n",
    "print(\"\\nCosine distance between cat and dog features:\")\n",
    "for i in range(len(cat_features)):\n",
    "    for j in range(len(dog_features)):\n",
    "        dist = cosine_distance(cat_features[i], dog_features[j])\n",
    "        print(f\"Cat {i+1} vs Dog {j+1}: {dist}\")\n",
    "\n",
    "print(\"\\nL2 norm between cat and dog features:\")\n",
    "for i in range(len(cat_features)):\n",
    "    for j in range(len(dog_features)):\n",
    "        dist = l2_norm(cat_features[i], dog_features[j])\n",
    "        print(f\"Cat {i+1} vs Dog {j+1}: {dist}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb2a08ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'strength'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18500/2408857932.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mshort_term_store\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m \u001b[0mshort_term_store\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Simulate decay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;31m# Simulate decay in long-term store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18500/2408857932.py\u001b[0m in \u001b[0;36mdecay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtrace\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrength\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecay_constant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrength\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'strength'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class MemoryTrace:\n",
    "    def __init__(self, feature, strength):\n",
    "        self.feature = feature\n",
    "        self.strength = strength\n",
    "\n",
    "class SensoryRegister:\n",
    "    def __init__(self, max_size=100):\n",
    "        self.max_size = max_size\n",
    "        self.queue = []\n",
    "\n",
    "    def push(self, memory_trace):\n",
    "        if len(self.queue) >= self.max_size:\n",
    "            self.queue.pop(0)  # Remove oldest memory trace\n",
    "        self.queue.append(memory_trace)\n",
    "\n",
    "    def pop(self):\n",
    "        if len(self.queue) == 0:\n",
    "            return None\n",
    "        return self.queue.pop(0)\n",
    "\n",
    "class ShortTermStore:\n",
    "    def __init__(self, max_capacity=7, decay_constant=0.05):\n",
    "        self.max_capacity = max_capacity\n",
    "        self.decay_constant = decay_constant\n",
    "        self.queue = []\n",
    "\n",
    "    def push(self, memory_trace):\n",
    "        if len(self.queue) >= self.max_capacity:\n",
    "            # Remove memory trace with lowest strength\n",
    "            self.queue.sort(key=lambda x: x.strength)\n",
    "            self.queue.pop(0)\n",
    "        self.queue.append(memory_trace)\n",
    "\n",
    "    def decay(self):\n",
    "        for trace in self.queue:\n",
    "            trace.strength -= self.decay_constant\n",
    "            if trace.strength <= 0:\n",
    "                self.queue.remove(trace)\n",
    "\n",
    "\n",
    "class LongTermStore:\n",
    "    def __init__(self, decay_constant=0.0005):\n",
    "        self.decay_constant = decay_constant\n",
    "        self.memory_traces = []\n",
    "\n",
    "    def push(self, memory_trace):\n",
    "        self.memory_traces.append(memory_trace)\n",
    "\n",
    "    def decay(self):\n",
    "        for trace in self.memory_traces:\n",
    "            trace.strength -= self.decay_constant\n",
    "            if trace.strength <= 0:\n",
    "                self.memory_traces.remove(trace)\n",
    "\n",
    "def information_decay(feature, decay_constant):\n",
    "    length = len(feature)\n",
    "    n = int(length * decay_constant)\n",
    "    idxes = random.sample(range(length), n)\n",
    "    for i in idxes:\n",
    "        feature[i] = random.uniform(0, 1)\n",
    "    return feature\n",
    "\n",
    "# Usage example\n",
    "# Assume cat_features and dog_features are lists of MemoryTrace objects\n",
    "\n",
    "# Simulate decay in short-term store\n",
    "short_term_store = ShortTermStore()\n",
    "for trace in cat_features + dog_features:\n",
    "    short_term_store.push(trace)\n",
    "\n",
    "short_term_store.decay()  # Simulate decay\n",
    "\n",
    "# Simulate decay in long-term store\n",
    "long_term_store = LongTermStore()\n",
    "for trace in cat_features + dog_features:\n",
    "    long_term_store.push(trace)\n",
    "\n",
    "long_term_store.decay()  # Simulate decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a193e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class MemoryTrace:\n",
    "    def __init__(self, feature, strength):\n",
    "        self.feature = feature\n",
    "        self.strength = strength\n",
    "\n",
    "class ShortTermStore:\n",
    "    def __init__(self, max_capacity=7, decay_constant=0.05):\n",
    "        self.max_capacity = max_capacity\n",
    "        self.decay_constant = decay_constant\n",
    "        self.queue = []\n",
    "\n",
    "    def push(self, memory_trace):\n",
    "        if len(self.queue) >= self.max_capacity:\n",
    "            # Remove memory trace with lowest strength\n",
    "            self.queue.sort(key=lambda x: x.strength)\n",
    "            self.queue.pop(0)\n",
    "        self.queue.append(memory_trace)\n",
    "\n",
    "    def decay(self):\n",
    "        for trace in self.queue:\n",
    "            trace.strength -= self.decay_constant\n",
    "            if trace.strength <= 0:\n",
    "                self.queue.remove(trace)\n",
    "\n",
    "class LongTermStore:\n",
    "    def __init__(self, decay_constant=0.0005):\n",
    "        self.decay_constant = decay_constant\n",
    "        self.memory_traces = []\n",
    "\n",
    "    def add_memory_trace(self, memory_trace):\n",
    "        self.memory_traces.append(memory_trace)\n",
    "\n",
    "    def decay(self):\n",
    "        for trace in self.memory_traces:\n",
    "            trace.strength -= self.decay_constant\n",
    "            if trace.strength <= 0:\n",
    "                self.memory_traces.remove(trace)\n",
    "\n",
    "class SensoryRegister:\n",
    "    def __init__(self, max_capacity=100):\n",
    "        self.max_capacity = max_capacity\n",
    "        self.queue = []\n",
    "\n",
    "    def perceive(self, sensory_info):\n",
    "        if len(self.queue) >= self.max_capacity:\n",
    "            self.queue.pop(0)\n",
    "        self.queue.append(sensory_info)\n",
    "\n",
    "# Create feature vectors (just placeholders for demonstration)\n",
    "cat_features = [np.random.rand(2048) for _ in range(6)]\n",
    "dog_features = [np.random.rand(2048) for _ in range(6)]\n",
    "\n",
    "# Create memory traces with random strengths\n",
    "cat_traces = [MemoryTrace(feature, random.uniform(0.5, 1.0)) for feature in cat_features]\n",
    "dog_traces = [MemoryTrace(feature, random.uniform(0.5, 1.0)) for feature in dog_features]\n",
    "\n",
    "# Create short-term store\n",
    "short_term_store = ShortTermStore()\n",
    "\n",
    "# Push memory traces to short-term store\n",
    "for trace in cat_traces + dog_traces:\n",
    "    short_term_store.push(trace)\n",
    "\n",
    "short_term_store.decay()  # Simulate decay\n",
    "\n",
    "# Create long-term store\n",
    "long_term_store = LongTermStore()\n",
    "\n",
    "# Add memory traces to long-term store\n",
    "for trace in cat_traces + dog_traces:\n",
    "    long_term_store.add_memory_trace(trace)\n",
    "\n",
    "long_term_store.decay()  # Simulate decay\n",
    "\n",
    "# Create sensory register\n",
    "sensory_register = SensoryRegister()\n",
    "\n",
    "# Perceive sensory information\n",
    "sensory_info = np.random.rand(100)  # Placeholder for sensory information\n",
    "sensory_register.perceive(sensory_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ccabd4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.models' has no attribute 'iRevNet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18500/238579746.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# Assume you have a 'features' variable containing the encoded features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# Initialize ImageRecovery class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mimage_recovery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageRecovery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# Recover images from features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18500/238579746.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# Load i-RevNet model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miRevNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_stride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Set model to evaluation mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchvision.models' has no attribute 'iRevNet'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "class ImageRecovery:\n",
    "    def __init__(self):\n",
    "        # Load i-RevNet model\n",
    "        self.model = models.iRevNet(num_classes=1000, layers=[4, 6, 8, 10], final_stride=2)\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    def recover_image(self, features):\n",
    "        # Flatten features\n",
    "        features_flat = features.flatten()\n",
    "        \n",
    "        # Reshape features to [6, 3072, 7, 7]\n",
    "        features_reshaped = features_flat.reshape((6, 3072, 7, 7))\n",
    "        \n",
    "        # Convert to tensor and normalize\n",
    "        features_tensor = torch.tensor(features_reshaped, dtype=torch.float32)\n",
    "        features_tensor /= 255.0  # Assuming features are in [0, 255] range\n",
    "        \n",
    "        # Recover images\n",
    "        with torch.no_grad():\n",
    "            recovered_images = self.model(features_tensor)\n",
    "        \n",
    "        return recovered_images.numpy()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume you have a 'features' variable containing the encoded features\n",
    "    # Initialize ImageRecovery class\n",
    "    image_recovery = ImageRecovery()\n",
    "\n",
    "    # Recover images from features\n",
    "    recovered_images = image_recovery.recover_image(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5008f05c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3392/956955751.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;31m# Initialize components\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m \u001b[0minjective_i_revnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIRevNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_blocks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m192\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m768\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3072\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[0mdecay_simulator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecaySimulation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecay_percentage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[0msensory_register\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSensoryRegister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_capacity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3392/956955751.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_layers, num_blocks, num_channels)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Limit loop iterations till layer 3 or num_layers (whichever is smaller)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m21\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m69\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m285\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownsampling_operators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDownsamplingOperator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Define the SplittingOperator module\n",
    "class SplittingOperator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SplittingOperator, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Define the DownsamplingOperator module\n",
    "class DownsamplingOperator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownsamplingOperator, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Define the BottleneckBlock module\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels // 4, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(out_channels // 4, out_channels // 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(out_channels // 4, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels // 4)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels // 4)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        return out\n",
    "\n",
    "# Define the iRevNet module\n",
    "class IRevNet(nn.Module):\n",
    "    def __init__(self, num_layers, num_blocks, num_channels):\n",
    "        super(IRevNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.splitting_operator = SplittingOperator(3, self.num_channels[0])\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.append(BottleneckBlock(self.num_channels[i % len(self.num_channels)], self.num_channels[i % len(self.num_channels)] * 4))\n",
    "\n",
    "        self.downsampling_operators = nn.ModuleList()\n",
    "        for i in range(1, num_layers):  # Limit loop iterations till layer 3 or num_layers (whichever is smaller)\n",
    "            if i == 3 or i == 21 or i == 69 or i == 285:\n",
    "                self.downsampling_operators.append(DownsamplingOperator(self.num_channels[i], self.num_channels[i]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.splitting_operator(x)\n",
    "        for i in range(self.num_blocks):\n",
    "            out = self.blocks[i](out)\n",
    "            if i + 1 in [3, 21, 69, 285]:\n",
    "                out = self.downsampling_operators[i // 3](out)\n",
    "        return out\n",
    "\n",
    "# Define the Decay Simulation module\n",
    "class DecaySimulation:\n",
    "    def __init__(self, decay_percentage):\n",
    "        self.decay_percentage = decay_percentage\n",
    "\n",
    "    def decay_feature(self, feature):\n",
    "        num_decay_elements = int(feature.size(0) * self.decay_percentage)\n",
    "        decay_indices = np.random.choice(feature.size(0), num_decay_elements, replace=False)\n",
    "        feature[decay_indices] = 0\n",
    "        return feature\n",
    "\n",
    "\n",
    "# Define the Sensory Register module\n",
    "class SensoryRegister:\n",
    "    def __init__(self, max_capacity):\n",
    "        self.max_capacity = max_capacity\n",
    "        self.queue = []\n",
    "\n",
    "    def push(self, feature):\n",
    "        if len(self.queue) < self.max_capacity:\n",
    "            self.queue.append(feature)\n",
    "        else:\n",
    "            self.queue.pop(0)\n",
    "            self.queue.append(feature)\n",
    "\n",
    "    def pop(self):\n",
    "        if len(self.queue) > 0:\n",
    "            return self.queue.pop(0)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Define the Short-term Store module\n",
    "class ShortTermStore:\n",
    "    def __init__(self, max_capacity):\n",
    "        self.max_capacity = max_capacity\n",
    "        self.queue = []\n",
    "\n",
    "    def push(self, feature):\n",
    "        if len(self.queue) < self.max_capacity:\n",
    "            self.queue.append(feature)\n",
    "        else:\n",
    "            self.queue.pop(0)\n",
    "            self.queue.append(feature)\n",
    "\n",
    "    def pop(self):\n",
    "        if len(self.queue) > 0:\n",
    "            return self.queue.pop(0)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def decay(self):\n",
    "        # Simulate decay process\n",
    "        for feature in self.queue:\n",
    "            # Decay the strength of each memory trace\n",
    "            feature -= 0.05  # Adjust the decay constant as needed\n",
    "            feature[feature < 0] = 0  # Ensure strength doesn't go negative\n",
    "\n",
    "    def recall(self, feature):\n",
    "        # Locate the closest memory trace and return\n",
    "        closest_feature = None\n",
    "        min_distance = float('inf')\n",
    "        for stored_feature in self.queue:\n",
    "            distance = torch.dist(feature, stored_feature)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_feature = stored_feature\n",
    "        return closest_feature\n",
    "\n",
    "# Define the Long-term Store module\n",
    "class LongTermStore:\n",
    "    def __init__(self):\n",
    "        self.memory_traces = []\n",
    "\n",
    "    def push(self, feature):\n",
    "        self.memory_traces.append(feature)\n",
    "\n",
    "    def pop(self):\n",
    "        if len(self.memory_traces) > 0:\n",
    "            return self.memory_traces.pop(0)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def decay(self):\n",
    "        # Simulate decay process\n",
    "        for feature in self.memory_traces:\n",
    "            # Decay the strength of each memory trace\n",
    "            feature -= 0.0005  # Adjust the decay constant as needed\n",
    "            feature[feature < 0] = 0  # Ensure strength doesn't go negative\n",
    "\n",
    "    def recall(self, feature):\n",
    "        # Locate the closest memory trace and return\n",
    "        closest_feature = None\n",
    "        min_distance = float('inf')\n",
    "        for stored_feature in self.memory_traces:\n",
    "            distance = torch.dist(feature, stored_feature)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_feature = stored_feature\n",
    "        return closest_feature\n",
    "\n",
    "# Reconstruction process\n",
    "class ImageRecovery:\n",
    "    def __init__(self, irevnet):\n",
    "        self.irevnet = irevnet\n",
    "\n",
    "    def recover_image(self, feature):\n",
    "        # Apply inverse iRevNet to recover the image\n",
    "        recovered_image = self.irevnet(feature)\n",
    "        return recovered_image\n",
    "\n",
    "# Initialize components\n",
    "injective_i_revnet = IRevNet(num_layers=300, num_blocks=100, num_channels=[48, 192, 768, 3072])\n",
    "decay_simulator = DecaySimulation(decay_percentage=0.1)\n",
    "sensory_register = SensoryRegister(max_capacity=100)\n",
    "short_term_store = ShortTermStore(max_capacity=7)\n",
    "long_term_store = LongTermStore()\n",
    "\n",
    "# Assuming cat features are provided\n",
    "cat_features = [torch.randn(2048) for _ in range(3)]\n",
    "\n",
    "# Assuming dog features are provided\n",
    "dog_features = [torch.randn(2048) for _ in range(3)]\n",
    "\n",
    "# Simulate processing cat features\n",
    "for cat_feature in cat_features:\n",
    "    cat_feature_decayed = decay_simulator.decay_feature(cat_feature)\n",
    "    sensory_register.push(cat_feature_decayed)\n",
    "\n",
    "# Simulate processing dog features\n",
    "for dog_feature in dog_features:\n",
    "    dog_feature_decayed = decay_simulator.decay_feature(dog_feature)\n",
    "    sensory_register.push(dog_feature_decayed)\n",
    "\n",
    "# Retrieve features from sensory register\n",
    "while sensory_register.queue:\n",
    "    feature = sensory_register.pop()\n",
    "    short_term_store.push(feature)\n",
    "\n",
    "# Simulate decay in short-term store\n",
    "short_term_store.pop()\n",
    "short_term_store.pop()\n",
    "short_term_store.decay()\n",
    "\n",
    "# Simulate decay in long-term store\n",
    "long_term_store.decay()\n",
    "\n",
    "# Reconstruction process\n",
    "image_recovery = ImageRecovery(injective_i_revnet)\n",
    "reconstructed_images = []\n",
    "for feature in short_term_store.queue + long_term_store.memory_traces:\n",
    "    recovered_image = image_recovery.recover_image(feature)\n",
    "    reconstructed_images.append(recovered_image)\n",
    "\n",
    "# Display the reconstructed images\n",
    "for i, image in enumerate(reconstructed_images):\n",
    "    print(f\"Reconstructed Image {i + 1}: {image.shape}\")  # Replace this with your display code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49f332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
