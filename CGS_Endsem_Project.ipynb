{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IK2jso_0IC0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
        "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# Load the pretrained ResNet-50 model\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "\n",
        "# Modify the model to use it as a feature extractor\n",
        "#Comment this when trying to compare with 2048 feature matrix\n",
        "#resnet50.avgpool = torch.nn.Identity() #Replace the avgpool layer with an Identity layer, which effectively does nothing\n",
        "resnet50.fc = torch.nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "resnet50.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY6CwF604R6H",
        "outputId": "6393468d-b079-4144-8ed6-155265aec9da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Identity()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "dataset = ImageFolder('/content/drive/MyDrive/Images', transform=transform)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "# Function to extract features\n",
        "def extract_features(model, data_loader):\n",
        "    model.eval()\n",
        "    features = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in data_loader:\n",
        "            output = model(inputs)\n",
        "            #shape of features\n",
        "            print(\"Shape of feature\", output.shape)\n",
        "            output = output.squeeze()  # Squeeze the output to [100352] or [2048]\n",
        "            features.append(output.cpu().numpy())\n",
        "\n",
        "    return np.array(features)\n",
        "\n",
        "# Extract features\n",
        "features = extract_features(resnet50, data_loader)"
      ],
      "metadata": {
        "id": "9FwBV5i20c_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a85081-d482-4457-aa6e-643aeeac9968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of feature torch.Size([1, 2048])\n",
            "Shape of feature torch.Size([1, 2048])\n",
            "Shape of feature torch.Size([1, 2048])\n",
            "Shape of feature torch.Size([1, 2048])\n",
            "Shape of feature torch.Size([1, 2048])\n",
            "Shape of feature torch.Size([1, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(features)\n",
        "print(len(features[0]))\n",
        "print(len(features[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eN4SmRV7-yy",
        "outputId": "1f32a133-f8af-438d-ee87-c12313f3cc97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.29308692 0.34076434 0.0606418  ... 0.5005807  0.19622953 0.16804181]\n",
            " [0.17378132 0.15040462 0.12912093 ... 0.29789406 0.12450052 0.17119877]\n",
            " [0.06553076 0.22242767 0.08264972 ... 0.27547953 0.13562112 0.14316227]\n",
            " [0.1805996  0.5667376  1.1920793  ... 0.64303255 0.03507868 0.10211677]\n",
            " [0.4666547  0.17642798 0.10143929 ... 0.37027115 0.11223205 0.10707459]\n",
            " [0.233478   0.5380204  0.5149275  ... 0.43153307 0.0828105  0.16118416]]\n",
            "2048\n",
            "2048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cosine distance and L2 norm of cat 1 with other images\n",
        "from scipy.spatial import distance\n",
        "\n",
        "def cosine_similarity(feature1, feature2):\n",
        "    # Normalize the feature vectors\n",
        "    norm1 = np.linalg.norm(feature1)\n",
        "    norm2 = np.linalg.norm(feature2)\n",
        "    # print(\"Normalized value 1 in cosine similarity\", norm1)\n",
        "    # print(\"Normalized value 2 in cosine similarity\",norm2)\n",
        "    # Compute cosine similarity as dot product divided by norms\n",
        "    sim = 1 - (np.dot(feature1, feature2) / (norm1 * norm2))\n",
        "    return sim\n",
        "\n",
        "def l2_norm(feature1, feature2):\n",
        "    return distance.euclidean(feature1, feature2)\n",
        "\n",
        "# Assuming 'features' is your numpy array of features from the previous code\n",
        "first_image_features = features[0]  # Features of the first image\n",
        "\n",
        "cosine_similarities = []\n",
        "l2_distances = []\n",
        "\n",
        "for feature in features[1:]:  # Start from the second element\n",
        "    cos_sim = cosine_similarity(first_image_features, feature)\n",
        "    l2_dist = l2_norm(first_image_features, feature)\n",
        "\n",
        "    cosine_similarities.append(cos_sim)\n",
        "    l2_distances.append(l2_dist)\n",
        "\n",
        "# Now you have cosine similarities and L2 norms in lists\n",
        "\n",
        "# Printing the results\n",
        "print(\"Cosine Distance with the first image:\")\n",
        "print(cosine_similarities)\n",
        "\n",
        "print(\"\\nL2 Distances with the first image:\")\n",
        "print(l2_distances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvKh-va-8fQY",
        "outputId": "608b9bd7-e076-4385-b12e-f556e4100dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Distance with the first image:\n",
            "[0.23807841539382935, 0.16347575187683105, 0.28057265281677246, 0.3209238648414612, 0.332078218460083]\n",
            "\n",
            "L2 Distances with the first image:\n",
            "[18.465999603271484, 15.076408386230469, 21.9773006439209, 23.492877960205078, 23.480634689331055]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def information_decay(F, D):\n",
        "    \"\"\"\n",
        "    Applies 'information decay' to a list of features by modifying a subset of its elements.\n",
        "\n",
        "    Parameters:\n",
        "    - F (list or np.array): The feature list to be modified.\n",
        "    - D (float): Decay constant used to determine the proportion of features to modify.\n",
        "\n",
        "    Returns:\n",
        "    - np.array: The modified feature list.\n",
        "    \"\"\"\n",
        "    L = len(F)  # Length of the feature list\n",
        "    N = int(L * D)  # Number of elements to modify\n",
        "    old_F=F\n",
        "\n",
        "    # Randomly sample N indexes from range 0 to L without replacement\n",
        "    idxes = np.random.choice(range(L), size=N, replace=False)\n",
        "\n",
        "    # Modify each selected index\n",
        "    for i in idxes:\n",
        "        # Assign a random floating-point number between 0 and 1\n",
        "        F[i] = np.random.rand()\n",
        "\n",
        "    print(\"Inside Information decay function\")\n",
        "    print(F==old_F)\n",
        "    return F"
      ],
      "metadata": {
        "id": "1u-7s6-28faD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sensory register code\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "class SensoryRegister:\n",
        "    def __init__(self, capacity=100):\n",
        "        # Initialize the sensory register with a fixed capacity\n",
        "        self.capacity = capacity\n",
        "        self.queue = deque(maxlen=capacity)\n",
        "\n",
        "    def push_information(self, data, info_type, attention):\n",
        "        \"\"\"\n",
        "        Pushes information into the sensory register. If the queue is full, the oldest information is automatically removed.\n",
        "\n",
        "        Parameters:\n",
        "        - data (any): The raw data of the information, e.g., feature vector from ResNet50.\n",
        "        - info_type (str): Type of the information (e.g., 'visual', 'auditory').\n",
        "        - attention (float): Attention value assigned to the information, where 0 < attention <= 1.\n",
        "\n",
        "        Returns:\n",
        "        - list: A list of information units that need to be transferred to short-term memory.\n",
        "        - In this case the type is Image since we are working with images only. Can be olfactory, audio etc.\n",
        "        \"\"\"\n",
        "        information_unit = {'data': data, 'type': info_type, 'attention': attention}\n",
        "        # Push new information to the queue\n",
        "        self.queue.append(information_unit)\n",
        "\n",
        "        # Transfer to short-term memory if attention is high\n",
        "        if attention > 0.5:\n",
        "\n",
        "            return information_unit\n",
        "\n",
        "\n",
        "\n",
        "        # There's no need to return anything if the information doesn't require transferring to STM\n",
        "        return None\n",
        "\n",
        "    def get_all_information(self):\n",
        "        \"\"\"\n",
        "        Returns a list of all current information in the sensory register.\n",
        "        \"\"\"\n",
        "        return list(self.queue)\n",
        "\n",
        "# Example Usage\n",
        "# sensory = SensoryRegister()\n",
        "# high_attention_data = sensory.push_information(data=features[0], info_type=\"visual\", attention=0.6)\n",
        "# sensory.push_information(data=features[1], info_type=\"visual\", attention=0.4)\n",
        "\n",
        "# print(\"High attention data (to transfer to STM):\", high_attention_data)\n",
        "# print(\"All current sensory information:\", sensory.get_all_information())\n"
      ],
      "metadata": {
        "id": "Jc7n1A1y8fhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#short term memory code\n",
        "\n",
        "import heapq\n",
        "\n",
        "class ShortTermStore:\n",
        "    def __init__(self, capacity=7, decay_rate=0.05):\n",
        "        self.capacity = capacity\n",
        "        self.decay_rate = decay_rate\n",
        "        self.memory = []\n",
        "        self.current_time = 0\n",
        "\n",
        "    def add_trace(self, data, strength):\n",
        "        if len(self.memory) >= self.capacity:\n",
        "            print(\"Removed trace due to insufficient capacity in short term store\")\n",
        "            heapq.heappop(self.memory)  # Remove the weakest trace\n",
        "        heapq.heappush(self.memory, (strength, self.current_time, data))\n",
        "        self.current_time += 1\n",
        "\n",
        "    def decay(self):\n",
        "         # Temporarily store decayed items to re-add them properly to the heap\n",
        "          temp_memory = []\n",
        "          while self.memory:\n",
        "              strength, time, data = heapq.heappop(self.memory)\n",
        "              new_strength = strength - self.decay_rate\n",
        "              if new_strength > 0:\n",
        "                if new_strength <0.5:\n",
        "                  print(\"Decaying memory trace in short term store\")\n",
        "                  old_data=data\n",
        "                  data=information_decay(data,self.decay_rate)\n",
        "                  print(data==old_data)\n",
        "                temp_memory.append((new_strength, time, data))\n",
        "\n",
        "          # Rebuild the heap from the decayed items\n",
        "          for item in temp_memory:\n",
        "              heapq.heappush(self.memory, item)\n",
        "\n",
        "    def rehearse(self, index):\n",
        "        # Rehearse a specific memory trace by index to increase its strength\n",
        "        if index < len(self.memory):\n",
        "            trace = list(heapq.heappop(self.memory))\n",
        "            trace[0] += 0.1  # Increase strength\n",
        "            heapq.heappush(self.memory, tuple(trace))\n",
        "\n",
        "    def transfer_to_lts(self, lts):\n",
        "        # Transfer to long-term store if strength exceeds 0.7\n",
        "        temp_memory=[]\n",
        "        while self.memory:\n",
        "          trace = heapq.heappop(self.memory)\n",
        "          if trace[0] > 0.7:\n",
        "            lts.add_trace(trace[2], trace[0])\n",
        "            print(f\"Transferred to LTS: {trace[2]}\")\n",
        "\n",
        "          else:\n",
        "            temp_memory.append(trace)\n",
        "\n",
        "        # Rebuild the heap from the non-transferred items\n",
        "        for item in temp_memory:\n",
        "            heapq.heappush(self.memory, item)\n",
        "        # for i in range(len(self.memory)):\n",
        "        #     if self.memory[i][0] > 0.7:\n",
        "        #         lts.add_trace(self.memory[i][2], self.memory[i][0])\n",
        "        #         heapq.heappop(self.memory[i])\n",
        "\n",
        "\n",
        "    def recall(self, new_feature, lts):\n",
        "        # Find the trace with the minimum cosine distance to new_feature\n",
        "        closest_match = None\n",
        "        min_distance = float('inf')\n",
        "        lts.recall(new_feature, 0.5, self)\n",
        "\n",
        "        for _, _, data in self.memory:\n",
        "            print(\"Length of data\", len(data))\n",
        "            distance = cosine_similarity(np.array(data), np.array(new_feature))\n",
        "            print(\"Distance during recall in short term store\", distance)\n",
        "            if distance < min_distance:\n",
        "                min_distance = distance\n",
        "                closest_match = data\n",
        "\n",
        "        return closest_match\n"
      ],
      "metadata": {
        "id": "R76nGILb8fn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#long term memory code\n",
        "\n",
        "# import heapq\n",
        "\n",
        "# class LongTermStore_1:\n",
        "#     def __init__(self, decay_rate=0.0005):\n",
        "#         self.memory = {}  # key: data, value: {'strength': float, 'connections': list, 'valid': bool}\n",
        "#         self.decay_rate = decay_rate\n",
        "\n",
        "#     def add_trace(self, data, strength, valid=True):\n",
        "#         self.memory[data] = {'strength': strength, 'connections': [], 'valid': valid}\n",
        "\n",
        "#     def connect_traces(self, data1, data2):\n",
        "#         if data1 in self.memory and data2 in self.memory:\n",
        "#             self.memory[data1]['connections'].append(data2)\n",
        "#             self.memory[data2]['connections'].append(data1)\n",
        "\n",
        "#     def decay(self):\n",
        "#         # Placeholder for using information_decay function to apply decay, adjusted for dictionary structure.\n",
        "\n",
        "#         \"\"\"\n",
        "#         Apply decay to each trace in the memory. If a trace's strength goes below zero,\n",
        "#         it will still be retained but could be considered for cleanup if needed.\n",
        "#         \"\"\"\n",
        "#         to_delete = []  # List to hold keys of traces to delete if necessary\n",
        "#         for data, details in list(self.memory.items()):\n",
        "#             # Reduce the strength\n",
        "#             new_strength = details['strength'] - self.decay_rate\n",
        "#             if new_strength > 0:\n",
        "#               details['strength'] = new_strength\n",
        "#               if(new_strength < 0.5):\n",
        "#                 details['data'] = information_decay(details['data'], self.decay_rate)\n",
        "\n",
        "#             else:\n",
        "#                 # Optionally, mark the trace as invalid or delete it\n",
        "#                 # details['valid'] = False\n",
        "#                 # Or directly delete the trace if it's not needed\n",
        "#                 to_delete.append(data)\n",
        "\n",
        "#         # Remove traces that have decayed completely (if you choose to delete them)\n",
        "#         for data in to_delete:\n",
        "#             del self.memory[data]\n",
        "\n",
        "\n",
        "\n",
        "#     def recall(self, feature, threshold, short_term_store):\n",
        "#         # Find the closest match based on a feature\n",
        "#         closest_match = None\n",
        "#         min_distance = float('inf')\n",
        "\n",
        "#         for data, details in self.memory.items():\n",
        "#             distance = np.linalg.norm(feature - np.array(data))  # Assuming 'data' is also a vector-like feature\n",
        "#             if distance < min_distance:\n",
        "#                 min_distance = distance\n",
        "#                 closest_match = data\n",
        "\n",
        "#         # Check if the closest match is valid and the distance is below the threshold\n",
        "#         if closest_match and self.memory[closest_match]['valid'] and min_distance < threshold:\n",
        "#             # Transfer the memory trace to the short-term store\n",
        "#             index=0\n",
        "#             self._transfer_trace(closest_match, short_term_store, threshold, index)\n",
        "\n",
        "#         return closest_match\n",
        "\n",
        "#     def _transfer_trace(self, data, short_term_store, threshold, index):\n",
        "#         # Recursively transfer the trace and its connected traces to the short-term store\n",
        "#         stack = [data]\n",
        "#         visited = set()\n",
        "\n",
        "#         while stack:\n",
        "#             current = stack.pop()\n",
        "#             if current not in visited:\n",
        "#                 visited.add(current)\n",
        "#                 trace_details = self.memory[current]\n",
        "#                 if trace_details['strength'] > threshold:\n",
        "#                     short_term_store.add_trace(current, trace_details['strength'])\n",
        "#                     # Rehearse the trace in STS to boost its strength, simulating memory reinforcement\n",
        "#                     short_term_store.rehearse(current,index)\n",
        "#                     index=index+1\n",
        "#                     #trace_details['strength'] += 0.1\n",
        "#                     # Add connected traces to the stack\n",
        "#                     for neighbor in trace_details['connections']:\n",
        "#                         stack.append(neighbor)\n",
        "\n"
      ],
      "metadata": {
        "id": "o6Q8rBszOCe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#long term memory code\n",
        "class LongTermStore:\n",
        "    def __init__(self, decay_rate=0.0005):\n",
        "        self.memory = []  # Now a list of dictionaries\n",
        "        self.decay_rate = decay_rate\n",
        "\n",
        "    def add_trace(self, data, strength, valid=True):\n",
        "        # Each trace is now appended as a dictionary to the list\n",
        "        self.memory.append({'data': data, 'strength': strength, 'connections': [], 'valid': valid})\n",
        "\n",
        "    def connect_traces(self, index1, index2):\n",
        "        # Connects traces by indices in the list\n",
        "        if index1 < len(self.memory) and index2 < len(self.memory):\n",
        "            self.memory[index1]['connections'].append(index2)\n",
        "            self.memory[index2]['connections'].append(index1)\n",
        "\n",
        "    def decay(self):\n",
        "        \"\"\"\n",
        "        Apply decay to each trace in the memory. If a trace's strength goes below zero,\n",
        "        it will still be retained but could be considered for cleanup if needed.\n",
        "        \"\"\"\n",
        "        to_delete = []\n",
        "        for i in range(len(self.memory)):\n",
        "            trace = self.memory[i]\n",
        "            new_strength = trace['strength'] - self.decay_rate\n",
        "            if new_strength > 0:\n",
        "                trace['strength'] = new_strength\n",
        "                if new_strength < 0.5:\n",
        "                    print(\"Decaying memory trace in long term store\")\n",
        "                    old_data=trace['data']\n",
        "                    trace['data'] = information_decay(trace['data'], self.decay_rate)\n",
        "                    print(old_data==trace['data'])\n",
        "            else:\n",
        "                to_delete.append(i)\n",
        "\n",
        "        # Remove traces that have decayed completely\n",
        "        for index in sorted(to_delete, reverse=True):\n",
        "            del self.memory[index]\n",
        "\n",
        "    def recall(self, feature, threshold, short_term_store):\n",
        "        # Find the closest match based on a feature\n",
        "        closest_match = None\n",
        "        min_distance = float('inf')\n",
        "        closest_index = -1\n",
        "\n",
        "        for i, trace in enumerate(self.memory):\n",
        "            distance = np.linalg.norm(feature - np.array(trace['data']))\n",
        "            if distance < min_distance:\n",
        "                min_distance = distance\n",
        "                closest_match = trace['data']\n",
        "                closest_index = i\n",
        "\n",
        "\n",
        "        if closest_match is not None:\n",
        "          # Check if the closest match is valid and the distance is below the threshold\n",
        "          if self.memory[closest_index]['valid'] and min_distance < threshold:\n",
        "              self._transfer_trace(closest_index, short_term_store, threshold)\n",
        "\n",
        "        return closest_match\n",
        "\n",
        "    def _transfer_trace(self, index, short_term_store, threshold):\n",
        "        # Recursively transfer the trace and its connected traces to the short-term store\n",
        "        stack = [index]\n",
        "        visited = set()\n",
        "\n",
        "        while stack:\n",
        "            current = stack.pop()\n",
        "            if current not in visited:\n",
        "                visited.add(current)\n",
        "                trace_details = self.memory[current]\n",
        "                if trace_details['strength'] > threshold:\n",
        "                    short_term_store.add_trace(trace_details['data'], trace_details['strength'])\n",
        "                    # Rehearse the trace in STS to boost its strength, simulating memory reinforcement\n",
        "                    short_term_store.rehearse(current)\n",
        "                    for neighbor in trace_details['connections']:\n",
        "                        stack.append(neighbor)\n"
      ],
      "metadata": {
        "id": "FnWdxfKGM30e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: use the above codes for enhancing it\n",
        "\n",
        "# Create the sensory register, short-term store, and long-term store\n",
        "sensory_register = SensoryRegister()\n",
        "short_term_store = ShortTermStore()\n",
        "long_term_store = LongTermStore()\n",
        "\n",
        "# Push information into the sensory register\n",
        "sensory_register.push_information(data=features[0], info_type=\"visual\", attention=0.6)\n",
        "\n",
        "# Transfer information from sensory register to short-term store\n",
        "information_unit = sensory_register.get_all_information()[0]\n",
        "if information_unit:\n",
        "    short_term_store.add_trace(data=information_unit['data'], strength=information_unit['attention'])\n",
        "\n",
        "# Apply decay to short-term store\n",
        "short_term_store.decay()\n",
        "\n",
        "# Transfer information from short-term store to long-term store\n",
        "short_term_store.transfer_to_lts(long_term_store)\n",
        "\n",
        "#print long term store memory\n",
        "print(\"Long-Term Store:\", long_term_store.memory)\n",
        "\n",
        "# Recall a memory trace based on a new feature from the long term store\n",
        "new_feature = features[1]\n",
        "print(\"Length of new features\",len(features[1]))\n",
        "threshold = 0.5\n",
        "long_term_store.recall(feature=new_feature, threshold=threshold, short_term_store=short_term_store)\n",
        "recovered_feature=short_term_store.recall(new_feature,long_term_store)\n",
        "\n",
        "print(\"Recovered Features\", recovered_feature)\n",
        "print(\"Size of recovered feature\", len(recovered_feature))\n",
        "\n",
        "# Print the current state of the memory stores\n",
        "print(\"Sensory Register:\", sensory_register.get_all_information())\n",
        "print(\"Short-Term Store:\", short_term_store.memory)\n",
        "print(\"Long-Term Store:\", long_term_store.memory)\n"
      ],
      "metadata": {
        "id": "eBOOTJkwOCwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df50c812-5434-4c88-8dbc-1f25dabb7639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Long-Term Store: []\n",
            "Length of new features 2048\n",
            "Length of data 2048\n",
            "Distance during recall in short term store 0.23807841539382935\n",
            "Recovered Features [0.29308692 0.34076434 0.0606418  ... 0.5005807  0.19622953 0.16804181]\n",
            "Size of recovered feature 2048\n",
            "Sensory Register: [{'data': array([0.29308692, 0.34076434, 0.0606418 , ..., 0.5005807 , 0.19622953,\n",
            "       0.16804181], dtype=float32), 'type': 'visual', 'attention': 0.6}]\n",
            "Short-Term Store: [(0.5499999999999999, 0, array([0.29308692, 0.34076434, 0.0606418 , ..., 0.5005807 , 0.19622953,\n",
            "       0.16804181], dtype=float32))]\n",
            "Long-Term Store: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/I_Revnet_zip/pytorch-i-revnet-master.zip' -d /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9idKgtsZyVn7",
        "outputId": "de8aa8cb-8517-4dcd-d9ef-8ff4acf8ef0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/I_Revnet_zip/pytorch-i-revnet-master.zip\n",
            "307413043e33540cbe9c3746ef420261f8138315\n",
            "   creating: /content/pytorch-i-revnet-master/\n",
            "  inflating: /content/pytorch-i-revnet-master/.gitignore  \n",
            "  inflating: /content/pytorch-i-revnet-master/CIFAR_main.py  \n",
            "  inflating: /content/pytorch-i-revnet-master/ILSVRC_main.py  \n",
            "  inflating: /content/pytorch-i-revnet-master/LICENSE  \n",
            "  inflating: /content/pytorch-i-revnet-master/README.md  \n",
            "   creating: /content/pytorch-i-revnet-master/checkpoint/\n",
            "   creating: /content/pytorch-i-revnet-master/checkpoint/ilsvrc2012/\n",
            "   creating: /content/pytorch-i-revnet-master/checkpoint/ilsvrc2012/pre-trained/\n",
            "  inflating: /content/pytorch-i-revnet-master/checkpoint/ilsvrc2012/pre-trained/info.txt  \n",
            "   creating: /content/pytorch-i-revnet-master/imgs/\n",
            "  inflating: /content/pytorch-i-revnet-master/imgs/algorithm.jpg  \n",
            "  inflating: /content/pytorch-i-revnet-master/imgs/inverted_val_samples.jpg  \n",
            "   creating: /content/pytorch-i-revnet-master/models/\n",
            "  inflating: /content/pytorch-i-revnet-master/models/iRevNet.py  \n",
            "  inflating: /content/pytorch-i-revnet-master/models/model_utils.py  \n",
            "  inflating: /content/pytorch-i-revnet-master/models/tests.py  \n",
            "  inflating: /content/pytorch-i-revnet-master/models/utils_cifar.py  \n",
            "   creating: /content/pytorch-i-revnet-master/scripts/\n",
            "  inflating: /content/pytorch-i-revnet-master/scripts/evaluate_ilsvrc-2012.sh  \n",
            "  inflating: /content/pytorch-i-revnet-master/scripts/invert_ilsvrc-2012.sh  \n",
            "  inflating: /content/pytorch-i-revnet-master/scripts/train_cifar-10.sh  \n",
            "  inflating: /content/pytorch-i-revnet-master/scripts/train_ilsvrc-2012.sh  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X8WgGEBPCLIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_features(features):\n",
        "    return np.array(features.reshape(-1))\n",
        "\n",
        "# Assuming sensory_register is an instance of SensoryRegister\n",
        "def process_through_sensory(flattened_features, sensory_register, attention):\n",
        "    #flattened_features = flatten_features(features)\n",
        "    sensory_register.push_information(data=flattened_features, info_type='visual', attention=attention)\n",
        "\n",
        "\n",
        "# This function simulates the decision-making for transferring data between the registers\n",
        "def update_memory_models(sensory_register, short_term_store, long_term_store):\n",
        "    for info in sensory_register.get_all_information():\n",
        "        if info['attention'] > 0.5:\n",
        "            short_term_store.add_trace(info['data'], info['attention'])\n",
        "        # Assume some condition for transfer to LTS or added complexity\n",
        "        if info['attention'] > 0.7:\n",
        "            # Transfer information from short-term store to long-term store\n",
        "            short_term_store.transfer_to_lts(long_term_store)\n",
        "\n",
        "\n",
        "# Periodic update functions to decay memory strengths\n",
        "def decay_memory_stores(short_term_store, long_term_store):\n",
        "    short_term_store.decay()\n",
        "    long_term_store.decay()\n",
        "\n",
        "\n",
        "def recover_image(i_revnet_model, processed_features):\n",
        "    \"\"\"\n",
        "    Uses the i-RevNet model to recover an image from the processed features.\n",
        "\n",
        "    Parameters:\n",
        "    - i_revnet_model (torch.nn.Module): The trained i-RevNet model for image recovery.\n",
        "    - processed_features (torch.Tensor): The reshaped features from which to recover the image.\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: The recovered image.\n",
        "    \"\"\"\n",
        "    # Ensure the model is in evaluation mode\n",
        "    i_revnet_model.eval()\n",
        "\n",
        "    # Check if CUDA is available and move the features to the appropriate device\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # i_revnet_model = i_revnet_model.to(device)\n",
        "    # processed_features = processed_features.to(device)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    with torch.no_grad():  # Ensure no gradients are computed in this operation\n",
        "        recovered_image = i_revnet_model(processed_features)\n",
        "\n",
        "    # The recovered_image tensor can be processed further if necessary, e.g., post-processing steps, clipping values, etc.\n",
        "    return recovered_image\n",
        "\n",
        "def recall_and_recover(long_term_store, recall_feature, threshold, short_term_store):\n",
        "    recalled_data = short_term_store.recall(recall_feature,long_term_store)\n",
        "    if len(recalled_data)>0:\n",
        "        #reshaped_features = reshape_features(np.array(recalled_data))\n",
        "        # recovered_image = recover_image(i_revnet_model, reshaped_features)\n",
        "        # return recovered_image\n",
        "        return recalled_data\n",
        "    return None\n",
        "\n",
        "def reshape_features(features):\n",
        "    return features.reshape(1, 2048, 7, 7)\n"
      ],
      "metadata": {
        "id": "xd7cv0KCDePU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compiling all steps together\n",
        "#doing this for 1 feature for now\n",
        "#flattening features[0] already done\n",
        "\n",
        "# Extract features\n",
        "features_new = extract_features(resnet50, data_loader)\n",
        "\n",
        "# Create the sensory register, short-term store, and long-term store. Give own decay rates as required\n",
        "sensory_register_final = SensoryRegister()\n",
        "short_term_store_final = ShortTermStore(decay_rate=0.05)\n",
        "long_term_store_final = LongTermStore(decay_rate=0.0005)\n",
        "\n",
        "#Push new feature as memory to memory model\n",
        "process_through_sensory(features_new[0],sensory_register_final,0.53)\n",
        "process_through_sensory(features_new[1],sensory_register_final,0.5)\n",
        "process_through_sensory(features_new[2],sensory_register_final,0.56)\n",
        "process_through_sensory(features_new[3],sensory_register_final,0.75)\n",
        "process_through_sensory(features_new[4],sensory_register_final,0.2)\n",
        "process_through_sensory(features_new[4],sensory_register_final,0.1)\n",
        "\n",
        "\n",
        "\n",
        "#print content stored in sensory register\n",
        "print(sensory_register_final.get_all_information())\n",
        "#Update short term store and long term store to have the trace\n",
        "update_memory_models(sensory_register_final,short_term_store_final,long_term_store_final)\n",
        "\n",
        "#print memories in short term store\n",
        "print(\"Short-Term Store:\", short_term_store_final.memory)\n",
        "print(\"Long-Term Store:\", long_term_store_final.memory)\n",
        "#Decay the memroies in the short term store and long term store\n",
        "decay_memory_stores(short_term_store_final,long_term_store_final)\n",
        "\n",
        "#print memories in short term store\n",
        "print(\"Short-Term Store:\", short_term_store_final.memory)\n",
        "print(\"Long-Term Store:\", long_term_store_final.memory)\n",
        "\n",
        "\n",
        "#Recall the memory from short term store when again the same feature is seen\n",
        "encoded_features=recall_and_recover(long_term_store_final,features_new[0],0.5,short_term_store)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MWg_-JeCLRv",
        "outputId": "8d4a7cfb-7846-42ab-9fd9-cc65fe5f8d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of feature torch.Size([1, 2048])\n",
            "Shape of feature torch.Size([1, 2048])\n",
            "Shape of feature torch.Size([1, 2048])\n",
            "Shape of feature torch.Size([1, 2048])\n",
            "Shape of feature torch.Size([1, 2048])\n",
            "Shape of feature torch.Size([1, 2048])\n",
            "[{'data': array([0.29308692, 0.34076434, 0.0606418 , ..., 0.5005807 , 0.19622953,\n",
            "       0.16804181], dtype=float32), 'type': 'visual', 'attention': 0.53}, {'data': array([0.17378132, 0.15040462, 0.12912093, ..., 0.29789406, 0.12450052,\n",
            "       0.17119877], dtype=float32), 'type': 'visual', 'attention': 0.5}, {'data': array([0.06553076, 0.22242767, 0.08264972, ..., 0.27547953, 0.13562112,\n",
            "       0.14316227], dtype=float32), 'type': 'visual', 'attention': 0.56}, {'data': array([0.1805996 , 0.5667376 , 1.1920793 , ..., 0.64303255, 0.03507868,\n",
            "       0.10211677], dtype=float32), 'type': 'visual', 'attention': 0.75}, {'data': array([0.4666547 , 0.17642798, 0.10143929, ..., 0.37027115, 0.11223205,\n",
            "       0.10707459], dtype=float32), 'type': 'visual', 'attention': 0.2}, {'data': array([0.4666547 , 0.17642798, 0.10143929, ..., 0.37027115, 0.11223205,\n",
            "       0.10707459], dtype=float32), 'type': 'visual', 'attention': 0.1}]\n",
            "Transferred to LTS: [0.1805996  0.5667376  1.1920793  ... 0.64303255 0.03507868 0.10211677]\n",
            "Short-Term Store: [(0.53, 0, array([0.29308692, 0.34076434, 0.0606418 , ..., 0.5005807 , 0.19622953,\n",
            "       0.16804181], dtype=float32)), (0.56, 1, array([0.06553076, 0.22242767, 0.08264972, ..., 0.27547953, 0.13562112,\n",
            "       0.14316227], dtype=float32))]\n",
            "Long-Term Store: [{'data': array([0.1805996 , 0.5667376 , 1.1920793 , ..., 0.64303255, 0.03507868,\n",
            "       0.10211677], dtype=float32), 'strength': 0.75, 'connections': [], 'valid': True}]\n",
            "Decaying memory trace in short term store\n",
            "Inside Information decay function\n",
            "[ True  True  True ...  True  True  True]\n",
            "[ True  True  True ...  True  True  True]\n",
            "Short-Term Store: [(0.48000000000000004, 0, array([0.29308692, 0.34076434, 0.0606418 , ..., 0.5005807 , 0.19622953,\n",
            "       0.16804181], dtype=float32)), (0.51, 1, array([0.06553076, 0.22242767, 0.08264972, ..., 0.27547953, 0.13562112,\n",
            "       0.14316227], dtype=float32))]\n",
            "Long-Term Store: [{'data': array([0.1805996 , 0.5667376 , 1.1920793 , ..., 0.64303255, 0.03507868,\n",
            "       0.10211677], dtype=float32), 'strength': 0.7495, 'connections': [], 'valid': True}]\n",
            "Length of data 2048\n",
            "Distance during recall in short term store 0.023495972156524658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded_features.shape)\n",
        "# encoded_features_flatten=flatten_features(encoded_features)\n",
        "# features_array=np.array(features[0])\n",
        "# features_flatten=flatten_features(features_array)\n",
        "#cosine similarity of encoded features retrieved and input feature\n",
        "print(encoded_features)\n",
        "print(features_new[0])\n",
        "cos_dist_encode= cosine_similarity(encoded_features,features_new[0])\n",
        "print(\"Cosine distance of retrieved memory trace with input trace 1(cat)\", cos_dist_encode)\n",
        "\n",
        "#L2 Norm of encoded features retrieved and input feature\n",
        "l2_norm_encode=l2_norm(encoded_features,features_new[0])\n",
        "print(\"L2 Norm of retrieved memory trace with input trace 1(cat)\", l2_norm_encode)\n",
        "\n",
        "\n",
        "#Similarity of retrieved with dog input images\n",
        "cos_dist_encode2=cosine_similarity(encoded_features,features_new[5])\n",
        "print(\"Cosine distance of retrieved memory trace with input trace 5(dog)\", cos_dist_encode2)\n",
        "\n",
        "l2_norm_encode2=l2_norm(encoded_features,features_new[5])\n",
        "print(\"L2 Norm of retrieved memory trace with input trace 5(dog)\", l2_norm_encode2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5ysueTqCLbL",
        "outputId": "07a93249-b809-4700-e18a-55dde664fac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2048,)\n",
            "[0.29308692 0.34076434 0.0606418  ... 0.5005807  0.19622953 0.16804181]\n",
            "[0.29308692 0.34076434 0.0606418  ... 0.5005807  0.19622953 0.16804181]\n",
            "Cosine distance of retrieved memory trace with input trace 1(cat) 0.023495972156524658\n",
            "L2 Norm of retrieved memory trace with input trace 1(cat) 5.869805335998535\n",
            "Cosine distance of retrieved memory trace with input trace 5(dog) 0.332078218460083\n",
            "L2 Norm of retrieved memory trace with input trace 5(dog) 23.480634689331055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/pytorch-i-revnet-master/ILSVRC_main.py --data encoded_features --nBlocks 6 16 72 6 --nStrides 2 2 2 2 \\\n",
        "                      --nChannels 24 96 384 1536 --init_ds 2 \\\n",
        "                      --resume /content/drive/MyDrive/I_Revnet_zip/ILSVRC_trained_irevnet.pth.tar --invert"
      ],
      "metadata": {
        "id": "cevO80DnCLk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I_Revnet Model\n",
        "\n",
        "\"\"\"\n",
        "Code for \"i-RevNet: Deep Invertible Networks\"\n",
        "https://openreview.net/pdf?id=HJsjkMb0Z\n",
        "ICLR, 2018\n",
        "\n",
        "(c) Joern-Henrik Jacobsen, 2018\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "#from .model_utils import split, merge, injective_pad, psi\n",
        "\n",
        "\n",
        "class irevnet_block(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, stride=1, first=False, dropout_rate=0.,\n",
        "                 affineBN=True, mult=4):\n",
        "        \"\"\" buid invertible bottleneck block \"\"\"\n",
        "        super(irevnet_block, self).__init__()\n",
        "        self.first = first\n",
        "        self.pad = 2 * out_ch - in_ch\n",
        "        self.stride = stride\n",
        "        self.inj_pad = injective_pad(self.pad)\n",
        "        self.psi = psi(stride)\n",
        "        if self.pad != 0 and stride == 1:\n",
        "            in_ch = out_ch * 2\n",
        "            print('')\n",
        "            print('| Injective iRevNet |')\n",
        "            print('')\n",
        "        layers = []\n",
        "        if not first:\n",
        "            layers.append(nn.BatchNorm2d(in_ch//2, affine=affineBN))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "        layers.append(nn.Conv2d(in_ch//2, int(out_ch//mult), kernel_size=3,\n",
        "                      stride=stride, padding=1, bias=False))\n",
        "        layers.append(nn.BatchNorm2d(int(out_ch//mult), affine=affineBN))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "        layers.append(nn.Conv2d(int(out_ch//mult), int(out_ch//mult),\n",
        "                      kernel_size=3, padding=1, bias=False))\n",
        "        layers.append(nn.Dropout(p=dropout_rate))\n",
        "        layers.append(nn.BatchNorm2d(int(out_ch//mult), affine=affineBN))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "        layers.append(nn.Conv2d(int(out_ch//mult), out_ch, kernel_size=3,\n",
        "                      padding=1, bias=False))\n",
        "        self.bottleneck_block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" bijective or injective block forward \"\"\"\n",
        "        if self.pad != 0 and self.stride == 1:\n",
        "            x = merge(x[0], x[1])\n",
        "            x = self.inj_pad.forward(x)\n",
        "            x1, x2 = split(x)\n",
        "            x = (x1, x2)\n",
        "        x1 = x[0]\n",
        "        x2 = x[1]\n",
        "        Fx2 = self.bottleneck_block(x2)\n",
        "        if self.stride == 2:\n",
        "            x1 = self.psi.forward(x1)\n",
        "            x2 = self.psi.forward(x2)\n",
        "        y1 = Fx2 + x1\n",
        "        return (x2, y1)\n",
        "\n",
        "    def inverse(self, x):\n",
        "        \"\"\" bijective or injecitve block inverse \"\"\"\n",
        "        x2, y1 = x[0], x[1]\n",
        "        if self.stride == 2:\n",
        "            x2 = self.psi.inverse(x2)\n",
        "        Fx2 = - self.bottleneck_block(x2)\n",
        "        x1 = Fx2 + y1\n",
        "        if self.stride == 2:\n",
        "            x1 = self.psi.inverse(x1)\n",
        "        if self.pad != 0 and self.stride == 1:\n",
        "            x = merge(x1, x2)\n",
        "            x = self.inj_pad.inverse(x)\n",
        "            x1, x2 = split(x)\n",
        "            x = (x1, x2)\n",
        "        else:\n",
        "            x = (x1, x2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class iRevNet(nn.Module):\n",
        "    def __init__(self, nBlocks, nStrides, nClasses, nChannels=None, init_ds=2,\n",
        "                 dropout_rate=0., affineBN=True, in_shape=None, mult=4):\n",
        "        super(iRevNet, self).__init__()\n",
        "        self.ds = in_shape[2]//2**(nStrides.count(2)+init_ds//2)\n",
        "        self.init_ds = init_ds\n",
        "        self.in_ch = in_shape[0] * 2**self.init_ds\n",
        "        self.nBlocks = nBlocks\n",
        "        self.first = True\n",
        "\n",
        "        print('')\n",
        "        print(' == Building iRevNet %d == ' % (sum(nBlocks) * 3 + 1))\n",
        "        if not nChannels:\n",
        "            nChannels = [self.in_ch//2, self.in_ch//2 * 4,\n",
        "                         self.in_ch//2 * 4**2, self.in_ch//2 * 4**3]\n",
        "\n",
        "        self.init_psi = psi(self.init_ds)\n",
        "        self.stack = self.irevnet_stack(irevnet_block, nChannels, nBlocks,\n",
        "                                        nStrides, dropout_rate=dropout_rate,\n",
        "                                        affineBN=affineBN, in_ch=self.in_ch,\n",
        "                                        mult=mult)\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[-1]*2, momentum=0.9)\n",
        "        self.linear = nn.Linear(nChannels[-1]*2, nClasses)\n",
        "\n",
        "    def irevnet_stack(self, _block, nChannels, nBlocks, nStrides, dropout_rate,\n",
        "                      affineBN, in_ch, mult):\n",
        "        \"\"\" Create stack of irevnet blocks \"\"\"\n",
        "        block_list = nn.ModuleList()\n",
        "        strides = []\n",
        "        channels = []\n",
        "        for channel, depth, stride in zip(nChannels, nBlocks, nStrides):\n",
        "            strides = strides + ([stride] + [1]*(depth-1))\n",
        "            channels = channels + ([channel]*depth)\n",
        "        for channel, stride in zip(channels, strides):\n",
        "            block_list.append(_block(in_ch, channel, stride,\n",
        "                                     first=self.first,\n",
        "                                     dropout_rate=dropout_rate,\n",
        "                                     affineBN=affineBN, mult=mult))\n",
        "            in_ch = 2 * channel\n",
        "            self.first = False\n",
        "        return block_list\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" irevnet forward \"\"\"\n",
        "        n = self.in_ch//2\n",
        "        if self.init_ds != 0:\n",
        "            x = self.init_psi.forward(x)\n",
        "        out = (x[:, :n, :, :], x[:, n:, :, :])\n",
        "        for block in self.stack:\n",
        "            out = block.forward(out)\n",
        "        out_bij = merge(out[0], out[1])\n",
        "        out = F.relu(self.bn1(out_bij))\n",
        "        out = F.avg_pool2d(out, self.ds)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out, out_bij\n",
        "\n",
        "    def inverse(self, out_bij):\n",
        "        \"\"\" irevnet inverse \"\"\"\n",
        "        out = split(out_bij)\n",
        "        for i in range(len(self.stack)):\n",
        "            out = self.stack[-1-i].inverse(out)\n",
        "        out = merge(out[0],out[1])\n",
        "        if self.init_ds != 0:\n",
        "            x = self.init_psi.inverse(out)\n",
        "        else:\n",
        "            x = out\n",
        "        return x\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = iRevNet(nBlocks=[6, 16, 72, 6], nStrides=[2, 2, 2, 2],\n",
        "                    nChannels=[24, 96, 384, 1536], nClasses=1000, init_ds=2,\n",
        "                    dropout_rate=0., affineBN=True, in_shape=[3, 224, 224],\n",
        "                    mult=4)\n",
        "    y = model(Variable(torch.randn(1, 3, 224, 224)))\n",
        "    #print(y.size())\n",
        "\n",
        "\n",
        "# Create an instance of the i_Revnet model\n",
        "i_revnet_model = iRevNet(nBlocks=[6, 16, 72, 6], nStrides=[2, 2, 2, 2],\n",
        "                    nChannels=[24, 96, 384, 1536], nClasses=1000, init_ds=2,\n",
        "                    dropout_rate=0., affineBN=True, in_shape=[3, 224, 224])\n",
        "\n",
        "\n",
        "# Load the weights from the .pth.tar file\n",
        "model_path = '/content/drive/MyDrive/I_Revnet_zip/ILSVRC_trained_irevnet.pth.tar'  # Change to the path of your model file\n",
        "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "i_revnet_model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "# If the file also contains optimizer state, you can load it similarly\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])"
      ],
      "metadata": {
        "id": "CozbREavmXXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "18bwGXwZmXnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_features(features):\n",
        "    return features.reshape(-1)\n",
        "\n",
        "# Assuming sensory_register is an instance of SensoryRegister\n",
        "def process_through_sensory(features, sensory_register, attention):\n",
        "    flattened_features = flatten_features(features)\n",
        "    sensory_register.push_information(data=flattened_features, info_type='visual', attention=attention)\n",
        "\n",
        "\n",
        "# This function simulates the decision-making for transferring data between the registers\n",
        "def update_memory_models(sensory_register, short_term_store, long_term_store):\n",
        "    for info in sensory_register.get_all_information():\n",
        "        if info['attention'] > 0.5:\n",
        "            short_term_store.add_trace(info['data'], info['attention'])\n",
        "        # Assume some condition for transfer to LTS or added complexity\n",
        "        if info['attention'] > 0.7:\n",
        "            long_term_store.add_trace(info['data'], info['attention'])\n",
        "\n",
        "\n",
        "# Periodic update functions to decay memory strengths\n",
        "def decay_memory_stores(short_term_store, long_term_store):\n",
        "    short_term_store.decay()\n",
        "    long_term_store.decay()\n",
        "\n",
        "\n",
        "def recover_image(i_revnet_model, processed_features):\n",
        "    \"\"\"\n",
        "    Uses the i-RevNet model to recover an image from the processed features.\n",
        "\n",
        "    Parameters:\n",
        "    - i_revnet_model (torch.nn.Module): The trained i-RevNet model for image recovery.\n",
        "    - processed_features (torch.Tensor): The reshaped features from which to recover the image.\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: The recovered image.\n",
        "    \"\"\"\n",
        "    # Ensure the model is in evaluation mode\n",
        "    i_revnet_model.eval()\n",
        "\n",
        "    # Check if CUDA is available and move the features to the appropriate device\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # i_revnet_model = i_revnet_model.to(device)\n",
        "    # processed_features = processed_features.to(device)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    with torch.no_grad():  # Ensure no gradients are computed in this operation\n",
        "        recovered_image = i_revnet_model(processed_features)\n",
        "\n",
        "    # The recovered_image tensor can be processed further if necessary, e.g., post-processing steps, clipping values, etc.\n",
        "    return recovered_image\n",
        "\n",
        "def recall_and_recover(i_revnet_model, long_term_store, recall_feature, threshold):\n",
        "    recalled_data = long_term_store.recall(recall_feature, threshold)\n",
        "    if recalled_data:\n",
        "        reshaped_features = reshape_features(np.array(recalled_data))\n",
        "        recovered_image = recover_image(i_revnet_model, reshaped_features)\n",
        "        return recovered_image\n",
        "    return None\n",
        "\n",
        "def reshape_features(features):\n",
        "    return features.reshape(6, 3072, 7, 7)\n"
      ],
      "metadata": {
        "id": "6umznCzHOCzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sqEa93U6OC2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QzUzN0vwOC5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importa Resnet 50 model\n",
        "base_model=ResNet50(weights='imagenet',include_top=False, input_shape=(img_width, img_height, 3))"
      ],
      "metadata": {
        "id": "al-uZg2205Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "id": "scAoX-Ol1AqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n0rRz3hD1OFV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}